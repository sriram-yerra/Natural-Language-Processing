{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install nltk\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4679c415",
   "metadata": {},
   "source": [
    "## **Spam or Ham Classifier Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5ca19",
   "metadata": {},
   "source": [
    "### **Importing the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fbae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "messages = pd.read_csv(\n",
    "    '../NLP-for-Transformers/Datum/SMSSpamCollection',\n",
    "    sep='\\t',\n",
    "    names=['label', 'message']\n",
    ")\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages['message'],'\\n')\n",
    "\n",
    "print(messages['message'].loc[100],'\\n')\n",
    "print(messages['message'].loc[451])\n",
    "\n",
    "'''\n",
    "1. messages refers to the pandas DataFrame that contains the dataset.\n",
    "\n",
    "2. ['message'] selects the \"message\" column from the DataFrame,\n",
    "   which contains the SMS/text content.\n",
    "\n",
    "3. .loc[451] selects the row with index label 451.\n",
    "\n",
    "4. This returns the text message present at index 451 in the dataset.\n",
    "\n",
    "5. It is commonly used to inspect or view a specific sample message\n",
    "   from the corpus for understanding or debugging.\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ea8ef",
   "metadata": {},
   "source": [
    "### **Step-1: Text Preprocessing** \n",
    "    1. Tokenization, \n",
    "    2. StopWords, \n",
    "    3. Stemmming, \n",
    "    4. Lemmatization, \n",
    "    5. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96864c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and preprocessing\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer # See the Capital letters\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f49c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) # List to Set\n",
    "'''\n",
    "The final corpus contains all messages after:\n",
    "    - Cleaning\n",
    "    - Lowercasing\n",
    "    - Stopword removal\n",
    "    - Stemming\n",
    "'''\n",
    "\n",
    "corpus = [] # Group of Sentences\n",
    "for i in range(len(messages)): # for each sentence in that message\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower().split()\n",
    "\n",
    "    cleaned_words = [] # Group of Individual Words\n",
    "    for word in review: # for each word in that sentence\n",
    "        if word not in stop_words:\n",
    "            s_word = ps.stem(word)\n",
    "            cleaned_words.append(s_word) # Adds the Stemmed word to the Sentence List\n",
    "\n",
    "    Sentence = ' '.join(cleaned_words) # Forming the sentance from Cleaned Words\n",
    "    corpus.append(Sentence) # Adds the cleaned sentence to the corpus List \n",
    "\n",
    "corpus\n",
    "\n",
    "'''\n",
    "1. corpus = [] initializes an empty list to store the cleaned and processed text data.\n",
    "\n",
    "2. stop_words stores the set of English stopwords from NLTK,\n",
    "   which will be removed from the text during preprocessing.\n",
    "\n",
    "3. The for loop iterates over each message in the dataset.\n",
    "\n",
    "4. re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "   - Replaces all non-alphabetic characters with spaces.\n",
    "   - Removes numbers, punctuation, and special symbols.\n",
    "\n",
    "5. review.lower().split()\n",
    "   - Converts text to lowercase.\n",
    "   - Splits the sentence into individual words (tokens).\n",
    "\n",
    "6. review1 = [] initializes a list to store processed words for the current message.\n",
    "\n",
    "7. For each word in review:\n",
    "   - If the word is not in stop_words,\n",
    "   - Apply Porter stemming using ps.stem(word),\n",
    "   - Append the stemmed word to review1.\n",
    "''';\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f0852",
   "metadata": {},
   "source": [
    "### **Step-2: Text --> Vectors** \n",
    "    1.BoW, \n",
    "    2. TF-IDF, \n",
    "    3. Word2Vec, \n",
    "    4. AvgWord2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29f20e",
   "metadata": {},
   "source": [
    "### **Creating the Bag of Words model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33ae8e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], shape=(5572, 2500))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features = 2500)\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "X = X.toarray()\n",
    "\n",
    "'''\n",
    "0. These below lines implement the Bag of Words (BoW) model on your text data.\n",
    "\n",
    "1. CountVectorizer is a tool from scikit-learn that converts raw text into \n",
    "numerical feature vectors using the Bag of Words (BoW) model.\n",
    "\n",
    "2. This creates a CountVectorizer object.\n",
    "    What it does:\n",
    "    Converts text â†’ numeric vectors\n",
    "    Builds a vocabulary from your corpus\n",
    "    Represents each document by word counts\n",
    "\n",
    "3. Keeps only the top 2500 most frequent words in the corpus.\n",
    "\n",
    "4. x = cv.fit_transform(corpus)\n",
    "This does two things:\n",
    "    ðŸ”¹ fit(corpus)\n",
    "        Scans all documents in corpus\n",
    "        Learns the vocabulary (up to 2500 words)\n",
    "        Assigns each word an index (column)\n",
    "\n",
    "    ðŸ”¹ transform(corpus)\n",
    "        Converts each document into a vector\n",
    "        Each vector length = vocabulary size (â‰¤ 2500)\n",
    "        Values = word counts in that document\n",
    "'''\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dbd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(messages['label']) # Dummies --> OHE.\n",
    "y = y.iloc[:,1] # selecting the 2nd column of the spam,ham\n",
    "\n",
    "'''\n",
    "0. get_dummies is a pandas function used to convert categorical values into numeric (one-hot encoded) columns.\n",
    "\n",
    "1. pd.get_dummies(messages['label']) converts the text labels (like \"ham\" and \"spam\")\n",
    "   into separate binary columns (one-hot encoding).\n",
    "\n",
    "2. Each label becomes a column with values 0 or 1.\n",
    "   Example:\n",
    "      \"ham\"  â†’ [1, 0]\n",
    "      \"spam\" â†’ [0, 1]\n",
    "\n",
    "3. y.iloc[:, 1] selects the second column from the dummy DataFrame,\n",
    "   which usually corresponds to the \"spam\" class.\n",
    "\n",
    "4. .values converts the selected column into a NumPy array.\n",
    "\n",
    "5. The final target vector y becomes:\n",
    "      spam â†’ 1\n",
    "      ham  â†’ 0\n",
    "\n",
    "6. This numeric y is used as the output/label variable\n",
    "   for training the machine learning classification model.\n",
    "'''\n",
    "\n",
    "# y = y.astype(int)\n",
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cddff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83349277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 2500)\n",
      "(1115, 2500)\n",
      "(4457,)\n",
      "(1115,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1db8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8952ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4f8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4877f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff40c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0cfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd760a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4d8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
