{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb53c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf987ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''\n",
    "Narendra Damodardas Modi[a] (born 17 September 1950) is an Indian politician who has served as the prime minister of India since 2014. Modi was the chief minister of Gujarat from 2001 to 2014 and is the member of parliament (MP) for Varanasi. He is a member of the Bharatiya Janata Party (BJP) and of the Rashtriya Swayamsevak Sangh (RSS), a right-wing Hindutva paramilitary volunteer organisation. He is the longest-serving prime minister outside the Indian National Congress.\n",
    "\n",
    "Modi was born and raised in Vadnagar, Bombay State (present-day Gujarat), where he completed his secondary education. He was introduced to the RSS at the age of eight, becoming a full-time worker for the organisation in Gujarat in 1971. The RSS assigned him to the BJP in 1985, and he rose through the party hierarchy, becoming general secretary in 1998.[b] In 2001, Modi was appointed chief minister of Gujarat and elected to the legislative assembly soon after. His administration is considered complicit in the 2002 Gujarat violence[c] and has been criticised for its management of the crisis. According to official records, a little over 1,000 people were killed, three-quarters of whom were Muslim; independent sources estimated 2,000 deaths, mostly Muslim.[4] A Special Investigation Team appointed by the Supreme Court of India in 2012 found no evidence to initiate prosecution proceedings against him, causing widespread anger and disbelief among the country's Muslim communities.[d] While his policies as chief minister were credited for encouraging economic growth, his administration was criticised for failing to significantly improve health, poverty and education indices in the state.[e]\n",
    "'''\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16bd7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74244c",
   "metadata": {},
   "source": [
    "## Text Preprocessing:\n",
    "\n",
    "### 1. Tokenisation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2037005",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')   # for newer NLTK\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data using Regular Expressions\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    # Remove everything except letters a-z and A-Z, replacing them with a space\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    # Convert text to lowercase\n",
    "    review = review.lower()\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592e8c9",
   "metadata": {},
   "source": [
    "### 2. Stemming and Lemmatisation\n",
    "\n",
    "The instructor demonstrates how to convert a large paragraph (corpus) into a list of sentences and then clean those sentences by removing special characters and converting everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee349aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# print(stop_words)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    review = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            review.append(lemma)\n",
    "\n",
    "    review = ' '.join(review)\n",
    "    corpus[i] = review\n",
    "\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167b39e",
   "metadata": {},
   "source": [
    "## Vectorisation: \n",
    "\n",
    "### 1. Binary Bag of Words (BBoW)\n",
    "\n",
    "Finally, the cleaned text is converted into numerical vectors using the CountVectorizer from Scikit-Learn. The instructor also mentions Binary Bag of Words, which only records if a word is present (1) or absent (0), regardless of its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8626c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise CountVectorizer\n",
    "cv = CountVectorizer(binary=True) # binary=True creates Binary Bag of Words [16]\n",
    "\n",
    "X_bbow = cv.fit_transform(corpus)\n",
    "\n",
    "# To view the mapping of words to their index:\n",
    "print(\"Vocabulary:\\n\", cv.vocabulary_)\n",
    "\n",
    "# Convert sparse matrix to array\n",
    "bbow_array = X_bbow.toarray()\n",
    "\n",
    "# Print full document-term matrix\n",
    "print(\"\\nDocument-Term Matrix:\")\n",
    "print(bbow_array)\n",
    "\n",
    "# Print vector for the first and Second sentence\n",
    "print(\"\\n First sentence vector:\")\n",
    "print(bbow_array[0])\n",
    "print(\"\\n Second sentence vector:\")\n",
    "print(bbow_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b1f25",
   "metadata": {},
   "source": [
    "### 2. Bag of Words (BoW)\n",
    "\n",
    "Bag of Words model, but instead of single words (unigrams), you’re using:\n",
    "\n",
    "bigrams (2-word sequences)\n",
    "\n",
    "trigrams (3-word sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the BoW model\n",
    "# ngram_range(2,3) considers both bigrams and trigrams [13]\n",
    "cv = CountVectorizer(ngram_range=(2, 3)) \n",
    "\n",
    "X_bow = cv.fit_transform(corpus)\n",
    "\n",
    "# Vocabulary shows the indexes of the features [4, 13]\n",
    "print(\"BoW Vocabulary:\", cv.vocabulary_)\n",
    "\n",
    "bow_array = X_bow.toarray()\n",
    "print(\"BoW Vectors:\\n\", bow_array)\n",
    "\n",
    "# Print vector for the first and Second sentence\n",
    "print(\"\\n First sentence vector:\")\n",
    "print(bow_array[0])\n",
    "print(\"\\n Second sentence vector:\")\n",
    "print(bow_array[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c8e11",
   "metadata": {},
   "source": [
    "### 3. Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF-IDF is used to capture word importance by giving higher weight to rare words and lower weight to common words that appear in every sentence. This helps address the lack of semantic meaning in BoW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e4e59",
   "metadata": {},
   "source": [
    "#### Key Concepts from the Sources\n",
    "• **Term Frequency (TF)** : Calculated as the number of repetitions of a word in a sentence divided by the total number of words in that sentence.\n",
    "\n",
    "• **Inverse Document Frequency (IDF)** : Calculated as the log of (total number of sentences / number of sentences containing the word).\n",
    "\n",
    "• **Sparsity Issue** : Both BoW and TF-IDF can result in large vectors containing many zeros if the vocabulary is huge, which makes computation difficult.\n",
    "\n",
    "• **Word Importance** : TF-IDF identifies important words; for example, if the word \"good\" appears in every sentence, its TF-IDF value becomes 0, indicating it does not help distinguish between sentences\n",
    "\n",
    "To understand TF-IDF, imagine a digital highlighter. If every sentence in a book has the word \"the\", your highlighter ignores it because it doesn't help you find a specific topic. However, if the word \"pizza\" only appears in one chapter, the highlighter marks it brightly because it is a rare and important keyword for that specific section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6bb923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising TF-IDF Vectorizer\n",
    "# max_features=10 selects the top 10 highest frequency features to reduce sparsity [19, 20]\n",
    "tfidf = TfidfVectorizer(max_features=10, ngram_range=(1, 1))\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "\n",
    "# Converting to an array to see the vector values [5]\n",
    "tfidf_array = X_tfidf.toarray()\n",
    "print(\"TF-IDF Vectors:\\n\", tfidf_array)\n",
    "\n",
    "# Print vector for the first and Second sentence\n",
    "print(\"\\n First sentence vector:\")\n",
    "print(tfidf_array[0])\n",
    "print(\"\\n Second sentence vector:\")\n",
    "print(tfidf_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c972d",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9df5a",
   "metadata": {},
   "source": [
    "### 1. Continous Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5778a8d",
   "metadata": {},
   "source": [
    "### 2. Skip - Gram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
