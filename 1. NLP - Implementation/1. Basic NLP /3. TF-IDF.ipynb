{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c827c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6648b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = '''\n",
    "Narendra Damodardas Modi[a] (born 17 September 1950) is an Indian politician who has served as the prime minister of India since 2014. Modi was the chief minister of Gujarat from 2001 to 2014 and is the member of parliament (MP) for Varanasi. He is a member of the Bharatiya Janata Party (BJP) and of the Rashtriya Swayamsevak Sangh (RSS), a right-wing Hindutva paramilitary volunteer organisation. He is the longest-serving prime minister outside the Indian National Congress.\n",
    "\n",
    "Modi was born and raised in Vadnagar, Bombay State (present-day Gujarat), where he completed his secondary education. He was introduced to the RSS at the age of eight, becoming a full-time worker for the organisation in Gujarat in 1971. The RSS assigned him to the BJP in 1985, and he rose through the party hierarchy, becoming general secretary in 1998.[b] In 2001, Modi was appointed chief minister of Gujarat and elected to the legislative assembly soon after. His administration is considered complicit in the 2002 Gujarat violence[c] and has been criticised for its management of the crisis. According to official records, a little over 1,000 people were killed, three-quarters of whom were Muslim; independent sources estimated 2,000 deaths, mostly Muslim.[4] A Special Investigation Team appointed by the Supreme Court of India in 2012 found no evidence to initiate prosecution proceedings against him, causing widespread anger and disbelief among the country's Muslim communities.[d] While his policies as chief minister were credited for encouraging economic growth, his administration was criticised for failing to significantly improve health, poverty and education indices in the state.[e]\n",
    "'''\n",
    "paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406771a",
   "metadata": {},
   "source": [
    "## Text Preprocessing:\n",
    "\n",
    "### 1. Tokenisation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')   # for newer NLTK\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c3bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data using Regular Expressions\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    # Remove everything except letters a-z and A-Z, replacing them with a space\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    # Convert text to lowercase\n",
    "    review = review.lower()\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa795ce",
   "metadata": {},
   "source": [
    "### 2. Stemming and Lemmatisation\n",
    "\n",
    "The instructor demonstrates how to convert a large paragraph (corpus) into a list of sentences and then clean those sentences by removing special characters and converting everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1bd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# print(stop_words)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "\n",
    "    review = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            review.append(lemma)\n",
    "\n",
    "    review = ' '.join(review)\n",
    "    corpus[i] = review\n",
    "\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede2a1c",
   "metadata": {},
   "source": [
    "## Vectorisation: \n",
    "\n",
    "### 1. Binary Bag of Words (BBoW)\n",
    "\n",
    "Finally, the cleaned text is converted into numerical vectors using the CountVectorizer from Scikit-Learn. The instructor also mentions Binary Bag of Words, which only records if a word is present (1) or absent (0), regardless of its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "deb5eeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'narendra': 78, 'damodardas': 35, 'modi': 74, 'born': 22, '17': 1, 'september': 104, '1950': 2, 'indian': 62, 'politician': 88, 'served': 105, 'prime': 91, 'minister': 73, 'india': 61, 'since': 108, '2014': 9, 'chief': 24, 'gujarat': 51, '2001': 6, 'member': 72, 'parliament': 84, 'mp': 76, 'varanasi': 120, 'he': 52, 'bharatiya': 19, 'janata': 66, 'party': 85, 'bjp': 20, 'rashtriya': 96, 'swayamsevak': 114, 'sangh': 101, 'rss': 100, 'right': 98, 'wing': 125, 'hindutva': 55, 'paramilitary': 83, 'volunteer': 122, 'organisation': 81, 'longest': 70, 'serving': 106, 'outside': 82, 'national': 79, 'congress': 28, 'raised': 95, 'vadnagar': 119, 'bombay': 21, 'state': 112, 'present': 90, 'day': 36, 'completed': 26, 'secondary': 102, 'education': 40, 'introduced': 64, 'age': 12, 'eight': 41, 'becoming': 18, 'full': 48, 'time': 118, 'worker': 126, '1971': 3, 'the': 116, 'assigned': 17, '1985': 4, 'rose': 99, 'hierarchy': 54, 'general': 49, 'secretary': 103, '1998': 5, 'in': 58, 'appointed': 15, 'elected': 42, 'legislative': 68, 'assembly': 16, 'soon': 109, 'his': 56, 'administration': 11, 'considered': 29, 'complicit': 27, '2002': 7, 'violence': 121, 'criticised': 34, 'management': 71, 'crisis': 33, 'according': 10, 'official': 80, 'record': 97, 'little': 69, '000': 0, 'people': 86, 'killed': 67, 'three': 117, 'quarters': 94, 'muslim': 77, 'independent': 59, 'source': 110, 'estimated': 44, 'death': 37, 'mostly': 75, 'special': 111, 'investigation': 65, 'team': 115, 'supreme': 113, 'court': 31, '2012': 8, 'found': 47, 'evidence': 45, 'initiate': 63, 'prosecution': 93, 'proceeding': 92, 'causing': 23, 'widespread': 124, 'anger': 14, 'disbelief': 38, 'among': 13, 'country': 30, 'community': 25, 'while': 123, 'policy': 87, 'credited': 32, 'encouraging': 43, 'economic': 39, 'growth': 50, 'failing': 46, 'significantly': 107, 'improve': 57, 'health': 53, 'poverty': 89, 'index': 60}\n",
      "\n",
      "Document-Term Matrix:\n",
      "[[0 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      " First sentence vector:\n",
      "[0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      " Second sentence vector:\n",
      "[0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Initialise CountVectorizer\n",
    "cv = CountVectorizer(binary=True) # binary=True creates Binary Bag of Words [16]\n",
    "\n",
    "X_bbow = cv.fit_transform(corpus)\n",
    "\n",
    "# To view the mapping of words to their index:\n",
    "print(\"Vocabulary:\\n\", cv.vocabulary_)\n",
    "\n",
    "# Convert sparse matrix to array\n",
    "bbow_array = X_bbow.toarray()\n",
    "\n",
    "# Print full document-term matrix\n",
    "print(\"\\nDocument-Term Matrix:\")\n",
    "print(bbow_array)\n",
    "\n",
    "# Print vector for the first and Second sentence\n",
    "print(\"\\n First sentence vector:\")\n",
    "print(bbow_array[0])\n",
    "print(\"\\n Second sentence vector:\")\n",
    "print(bbow_array[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60fee97",
   "metadata": {},
   "source": [
    "### 2. Bag of Words (BoW)\n",
    "\n",
    "Bag of Words model, but instead of single words (unigrams), you’re using:\n",
    "\n",
    "bigrams (2-word sequences)\n",
    "\n",
    "trigrams (3-word sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c217d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3e7c7250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Vocabulary: {'narendra damodardas': 188, 'damodardas modi': 76, 'modi born': 178, 'born 17': 51, '17 september': 4, 'september 1950': 244, '1950 indian': 6, 'indian politician': 144, 'politician served': 209, 'served prime': 246, 'prime minister': 215, 'minister india': 172, 'india since': 140, 'since 2014': 252, 'narendra damodardas modi': 189, 'damodardas modi born': 77, 'modi born 17': 179, 'born 17 september': 52, '17 september 1950': 5, 'september 1950 indian': 245, '1950 indian politician': 7, 'indian politician served': 145, 'politician served prime': 210, 'served prime minister': 247, 'prime minister india': 216, 'minister india since': 173, 'india since 2014': 141, 'modi chief': 181, 'chief minister': 57, 'minister gujarat': 169, 'gujarat 2001': 109, '2001 2014': 10, '2014 member': 18, 'member parliament': 165, 'parliament mp': 199, 'mp varanasi': 184, 'modi chief minister': 182, 'chief minister gujarat': 59, 'minister gujarat 2001': 170, 'gujarat 2001 2014': 110, '2001 2014 member': 11, '2014 member parliament': 19, 'member parliament mp': 166, 'parliament mp varanasi': 200, 'he member': 121, 'member bharatiya': 163, 'bharatiya janata': 43, 'janata party': 152, 'party bjp': 201, 'bjp rashtriya': 47, 'rashtriya swayamsevak': 226, 'swayamsevak sangh': 261, 'sangh rss': 240, 'rss right': 238, 'right wing': 230, 'wing hindutva': 280, 'hindutva paramilitary': 127, 'paramilitary volunteer': 197, 'volunteer organisation': 275, 'he member bharatiya': 122, 'member bharatiya janata': 164, 'bharatiya janata party': 44, 'janata party bjp': 153, 'party bjp rashtriya': 202, 'bjp rashtriya swayamsevak': 48, 'rashtriya swayamsevak sangh': 227, 'swayamsevak sangh rss': 262, 'sangh rss right': 241, 'rss right wing': 239, 'right wing hindutva': 231, 'wing hindutva paramilitary': 281, 'hindutva paramilitary volunteer': 128, 'paramilitary volunteer organisation': 198, 'he longest': 119, 'longest serving': 160, 'serving prime': 248, 'minister outside': 174, 'outside indian': 195, 'indian national': 142, 'national congress': 190, 'he longest serving': 120, 'longest serving prime': 161, 'serving prime minister': 249, 'prime minister outside': 217, 'minister outside indian': 175, 'outside indian national': 196, 'indian national congress': 143, 'born raised': 53, 'raised vadnagar': 224, 'vadnagar bombay': 271, 'bombay state': 49, 'state present': 257, 'present day': 213, 'day gujarat': 78, 'gujarat completed': 111, 'completed secondary': 60, 'secondary education': 242, 'modi born raised': 180, 'born raised vadnagar': 54, 'raised vadnagar bombay': 225, 'vadnagar bombay state': 272, 'bombay state present': 50, 'state present day': 258, 'present day gujarat': 214, 'day gujarat completed': 79, 'gujarat completed secondary': 112, 'completed secondary education': 61, 'he introduced': 117, 'introduced rss': 148, 'rss age': 234, 'age eight': 26, 'eight becoming': 88, 'becoming full': 39, 'full time': 102, 'time worker': 269, 'worker organisation': 282, 'organisation gujarat': 193, 'gujarat 1971': 108, 'he introduced rss': 118, 'introduced rss age': 149, 'rss age eight': 235, 'age eight becoming': 27, 'eight becoming full': 89, 'becoming full time': 40, 'full time worker': 103, 'time worker organisation': 270, 'worker organisation gujarat': 283, 'organisation gujarat 1971': 194, 'the rss': 265, 'rss assigned': 236, 'assigned bjp': 37, 'bjp 1985': 45, '1985 rose': 8, 'rose party': 232, 'party hierarchy': 203, 'hierarchy becoming': 125, 'becoming general': 41, 'general secretary': 104, 'secretary 1998': 243, 'the rss assigned': 266, 'rss assigned bjp': 237, 'assigned bjp 1985': 38, 'bjp 1985 rose': 46, '1985 rose party': 9, 'rose party hierarchy': 233, 'party hierarchy becoming': 204, 'hierarchy becoming general': 126, 'becoming general secretary': 42, 'general secretary 1998': 105, 'in 2001': 133, '2001 modi': 12, 'modi appointed': 176, 'appointed chief': 32, 'gujarat elected': 113, 'elected legislative': 90, 'legislative assembly': 156, 'assembly soon': 36, 'in 2001 modi': 134, '2001 modi appointed': 13, 'modi appointed chief': 177, 'appointed chief minister': 33, 'minister gujarat elected': 171, 'gujarat elected legislative': 114, 'elected legislative assembly': 91, 'legislative assembly soon': 157, 'his administration': 129, 'administration considered': 22, 'considered complicit': 64, 'complicit 2002': 62, '2002 gujarat': 14, 'gujarat violence': 115, 'violence criticised': 273, 'criticised management': 74, 'management crisis': 162, 'his administration considered': 130, 'administration considered complicit': 23, 'considered complicit 2002': 65, 'complicit 2002 gujarat': 63, '2002 gujarat violence': 15, 'gujarat violence criticised': 116, 'violence criticised management': 274, 'criticised management crisis': 75, 'according official': 20, 'official record': 191, 'record little': 228, 'little 000': 158, '000 people': 2, 'people killed': 205, 'killed three': 154, 'three quarters': 267, 'quarters muslim': 222, 'muslim independent': 186, 'independent source': 135, 'source estimated': 253, 'estimated 000': 94, '000 death': 0, 'death mostly': 80, 'mostly muslim': 183, 'according official record': 21, 'official record little': 192, 'record little 000': 229, 'little 000 people': 159, '000 people killed': 3, 'people killed three': 206, 'killed three quarters': 155, 'three quarters muslim': 268, 'quarters muslim independent': 223, 'muslim independent source': 187, 'independent source estimated': 136, 'source estimated 000': 254, 'estimated 000 death': 95, '000 death mostly': 1, 'death mostly muslim': 81, 'special investigation': 255, 'investigation team': 150, 'team appointed': 263, 'appointed supreme': 34, 'supreme court': 259, 'court india': 68, 'india 2012': 138, '2012 found': 16, 'found evidence': 100, 'evidence initiate': 96, 'initiate prosecution': 146, 'prosecution proceeding': 220, 'proceeding causing': 218, 'causing widespread': 55, 'widespread anger': 278, 'anger disbelief': 30, 'disbelief among': 82, 'among country': 28, 'country muslim': 66, 'muslim community': 185, 'special investigation team': 256, 'investigation team appointed': 151, 'team appointed supreme': 264, 'appointed supreme court': 35, 'supreme court india': 260, 'court india 2012': 69, 'india 2012 found': 139, '2012 found evidence': 17, 'found evidence initiate': 101, 'evidence initiate prosecution': 97, 'initiate prosecution proceeding': 147, 'prosecution proceeding causing': 221, 'proceeding causing widespread': 219, 'causing widespread anger': 56, 'widespread anger disbelief': 279, 'anger disbelief among': 31, 'disbelief among country': 83, 'among country muslim': 29, 'country muslim community': 67, 'while policy': 276, 'policy chief': 207, 'minister credited': 167, 'credited encouraging': 70, 'encouraging economic': 92, 'economic growth': 84, 'growth administration': 106, 'administration criticised': 24, 'criticised failing': 72, 'failing significantly': 98, 'significantly improve': 250, 'improve health': 131, 'health poverty': 123, 'poverty education': 211, 'education index': 86, 'index state': 137, 'while policy chief': 277, 'policy chief minister': 208, 'chief minister credited': 58, 'minister credited encouraging': 168, 'credited encouraging economic': 71, 'encouraging economic growth': 93, 'economic growth administration': 85, 'growth administration criticised': 107, 'administration criticised failing': 25, 'criticised failing significantly': 73, 'failing significantly improve': 99, 'significantly improve health': 251, 'improve health poverty': 132, 'health poverty education': 124, 'poverty education index': 212, 'education index state': 87}\n",
      "BoW Vectors:\n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      " First sentence vector:\n",
      "[0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      " Second sentence vector:\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Creating the BoW model\n",
    "# ngram_range(2,3) considers both bigrams and trigrams [13]\n",
    "cv = CountVectorizer(ngram_range=(2, 3)) \n",
    "\n",
    "X_bow = cv.fit_transform(corpus)\n",
    "\n",
    "# Vocabulary shows the indexes of the features [4, 13]\n",
    "print(\"BoW Vocabulary:\", cv.vocabulary_)\n",
    "\n",
    "bow_array = X_bow.toarray()\n",
    "print(\"BoW Vectors:\\n\", bow_array)\n",
    "\n",
    "# Print vector for the first and Second sentence\n",
    "print(\"\\n First sentence vector:\")\n",
    "print(bow_array[0])\n",
    "print(\"\\n Second sentence vector:\")\n",
    "print(bow_array[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f67acd",
   "metadata": {},
   "source": [
    "### 3. Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF-IDF is used to capture word importance by giving higher weight to rare words and lower weight to common words that appear in every sentence. This helps address the lack of semantic meaning in BoW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc730f1",
   "metadata": {},
   "source": [
    "#### Key Concepts from the Sources\n",
    "• **Term Frequency (TF)** : Calculated as the number of repetitions of a word in a sentence divided by the total number of words in that sentence.\n",
    "\n",
    "• **Inverse Document Frequency (IDF)** : Calculated as the log of (total number of sentences / number of sentences containing the word).\n",
    "\n",
    "• **Sparsity Issue** : Both BoW and TF-IDF can result in large vectors containing many zeros if the vocabulary is huge, which makes computation difficult.\n",
    "\n",
    "• **Word Importance** : TF-IDF identifies important words; for example, if the word \"good\" appears in every sentence, its TF-IDF value becomes 0, indicating it does not help distinguish between sentences\n",
    "\n",
    "To understand TF-IDF, imagine a digital highlighter. If every sentence in a book has the word \"the\", your highlighter ignores it because it doesn't help you find a specific topic. However, if the word \"pizza\" only appears in one chapter, the highlighter marks it brightly because it is a rare and important keyword for that specific section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef02489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "30375496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectors:\n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  0.67310944 0.73954289 0.         0.        ]\n",
      " [0.         0.         0.56285112 0.         0.46154597 0.\n",
      "  0.46154597 0.50709887 0.         0.        ]\n",
      " [0.         0.62345743 0.         0.         0.         0.5528566\n",
      "  0.         0.         0.         0.5528566 ]\n",
      " [0.         0.         0.         0.         0.         0.77326237\n",
      "  0.6340862  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.67310944 0.\n",
      "  0.         0.73954289 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.50161301 0.61171251\n",
      "  0.         0.         0.         0.61171251]\n",
      " [0.         0.74820002 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.66347324]\n",
      " [0.         0.         0.56285112 0.         0.46154597 0.\n",
      "  0.46154597 0.50709887 0.         0.        ]\n",
      " [0.         0.         0.         0.80878129 0.58810953 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.75729969 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.65306752 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.        ]\n",
      " [0.         0.         0.58280214 0.65722707 0.         0.\n",
      "  0.47790609 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "\n",
      " First sentence vector:\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.67310944 0.73954289 0.         0.        ]\n",
      "\n",
      " Second sentence vector:\n",
      "[0.         0.         0.56285112 0.         0.46154597 0.\n",
      " 0.46154597 0.50709887 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Initialising TF-IDF Vectorizer\n",
    "# max_features=10 selects the top 10 highest frequency features to reduce sparsity [19, 20]\n",
    "tfidf = TfidfVectorizer(max_features=10, ngram_range=(1, 1))\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(corpus)\n",
    "\n",
    "# Converting to an array to see the vector values [5]\n",
    "tfidf_array = X_tfidf.toarray()\n",
    "print(\"TF-IDF Vectors:\\n\", tfidf_array)\n",
    "\n",
    "# Print vector for the first and Second sentence\n",
    "print(\"\\n First sentence vector:\")\n",
    "print(tfidf_array[0])\n",
    "print(\"\\n Second sentence vector:\")\n",
    "print(tfidf_array[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
